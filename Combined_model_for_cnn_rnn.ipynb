{"cells":[{"cell_type":"markdown","source":["## import libraries"],"metadata":{"id":"jsYsmBD0hsbG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3lZOqn_wVh1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import GRU, Input, Dense, Activation, RepeatVector,BatchNormalization, Bidirectional, LSTM, Dropout, Embedding\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.models import Model\n","from keras.layers import Input, Reshape, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Embedding, Dropout, BatchNormalization\n","from sklearn.metrics import confusion_matrix\n","from nltk.tokenize import word_tokenize\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.utils import shuffle\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","\n","import pandas as pd\n","import re\n","import string\n","import csv\n","\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from keras.callbacks import ReduceLROnPlateau\n","import os\n","from keras import backend as K\n","import numpy as np\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from google.colab import files\n","import pandas as pd\n","\n","import pickle\n","import shutil\n","from nltk.corpus import stopwords\n","from textblob import TextBlob\n","from textblob import Word\n","import nltk\n","from PIL import Image"]},{"cell_type":"markdown","metadata":{"id":"vtVCij9tibtc"},"source":["#Preprocessing layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7Bvnd_FkUs6"},"outputs":[],"source":["dir =\"/content/drive/MyDrive/Emotion\""]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","\n","# Define the path to the directory containing the image folders\n","image_dir = dir\n","\n","# Initialize empty lists to store images and corresponding labels\n","images = []\n","labels = []\n","\n","# Get a sorted list of subdirectories in alphabetical order\n","subdirectories = sorted(os.listdir(image_dir))\n","\n","# Create a mapping from folder names to numerical labels\n","label_mapping = {}\n","\n","# Define the target shape for all images (e.g., 224x224 pixels for consistency)\n","target_shape = (224, 224)\n","\n","# Iterate through the subdirectories in alphabetical order\n","for index, label in enumerate(subdirectories):\n","    label_dir = os.path.join(image_dir, label)\n","\n","    # Check if it's a directory\n","    if os.path.isdir(label_dir):\n","        # Iterate through the files in the subdirectory\n","        for image_file in os.listdir(label_dir):\n","            image_path = os.path.join(label_dir, image_file)\n","\n","            # Load the image using OpenCV\n","            image = cv2.imread(image_path)\n","\n","            # Check if the image loaded successfully\n","            if image is not None:\n","                # Resize the image to the target shape (e.g., 224x224)\n","                image = cv2.resize(image, target_shape)\n","\n","                # Normalize the image by dividing by 255 to scale pixel values between 0 and 1\n","                image = image / 255.0\n","\n","                # Append the image and corresponding label to the lists\n","                images.append(image)\n","                labels.append(index)  # Use the index as the numerical label\n","                label_mapping[index] = label\n","\n","# Convert lists to NumPy arrays for further processing (if needed)\n","images = np.array(images)\n","labels = np.array(labels)\n","\n","# Now, you have your images with a common shape (e.g., 224x224) and normalized pixel values between 0 and 1.\n","# You can use label_mapping to get the original folder names.\n"],"metadata":{"id":"t_GmoyhK4lN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(labels)"],"metadata":{"id":"rKN--3tI8IUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3caZLB3G1hc"},"outputs":[],"source":["df = pd.read_csv(\"/content/drive/MyDrive/Emotion_csv/Emotion.csv\")\n","df['Clean_Text'] = df['Clean_Text'].astype(str)\n","data= df.iloc[1:8122]\n","data.head(4 )"]},{"cell_type":"code","source":["# Separate data by label\n","label_0_data = data[data['label'] == 0].sample(n=390, random_state=42)\n","label_1_data = data[data['label'] == 1].sample(n=390, random_state=42)\n","label_2_data = data[data['label'] == 2].sample(n=390, random_state=42)\n","\n","# Concatenate the balanced data for all labels\n","balanced_data = pd.concat([label_0_data, label_1_data, label_2_data])\n","\n","# Shuffle the balanced data\n","balanced_data = balanced_data.sample(frac=1, random_state=42)"],"metadata":{"id":"S_EgjzPtRec2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["balanced_data.head(3)"],"metadata":{"id":"hJiu1KZH1Vof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def preprocess_images_from_folder(main_folder, target_size=(48, 48), images_per_class=390):\n","    image_data = []\n","    labels = []\n","\n","    class_labels = sorted(os.listdir(main_folder))  # Get the class labels from folder names\n","\n","    for class_label in class_labels:\n","        class_folder = os.path.join(main_folder, class_label)\n","        images_loaded = 0  # Counter to track the number of loaded images per class\n","        for filename in os.listdir(class_folder):\n","            if images_loaded >= images_per_class:\n","                break  # Stop loading images for this class if the limit is reached\n","\n","            img_path = os.path.join(class_folder, filename)\n","            img = Image.open(img_path)\n","            img = img.resize(target_size)\n","            img_array = np.array(img)\n","            img_array = img_array.astype('float32') / 255.0\n","            image_data.append(img_array)\n","\n","            # Convert class label to an integer index\n","            label_index = class_labels.index(class_label)\n","            labels.append(label_index)\n","\n","            images_loaded += 1\n","\n","    image_data = np.array(image_data)\n","    labels = np.array(labels)\n","\n","    # Convert integer labels to one-hot encoded vectors\n","    labels_one_hot = to_categorical(labels)\n","\n","    return image_data, labels_one_hot\n","\n","# Replace 'main_folder' with the path to your main folder containing class folders\n","main_folder = dir\n","target_image_size = (48, 48)\n","images_per_class = 390\n","\n","# Preprocess limited number of images from the folder\n","image_data, labels_one_hot = preprocess_images_from_folder(main_folder, target_image_size, images_per_class)"],"metadata":{"id":"gjLT9sQvUo7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_one_hot.shape"],"metadata":{"id":"Fwk581JSj5xW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["happy_class_index = 0\n","happy_count = np.sum(labels_one_hot[:, happy_class_index])\n","print(\"Number of 'depression' images:\", happy_count)"],"metadata":{"id":"rclFlPPIlQiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_text_data(text_data, labels, max_sequence_length):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(text_data)\n","\n","    # Convert text data to sequences of integer indices\n","    sequences = tokenizer.texts_to_sequences(text_data)\n","\n","    # Pad sequences to a fixed length\n","    padded_sequences = pad_sequences(sequences,\n","    maxlen=max_sequence_length,\n","    padding='pre',\n","    truncating='pre')\n","\n","    max_index = np.max(padded_sequences)\n","    vocabulary_size = max_index + 1\n","\n","\n","    # Convert class labels to one-hot encoded vectors\n","    labels_one_hot = to_categorical(labels)\n","\n","    return padded_sequences, labels_one_hot, vocabulary_size\n","\n","\n","text_data = balanced_data[\"Clean_Text\"]\n","labels =balanced_data[\"label\"]\n","max_sequence_length = 31\n","# Preprocess the text data and combine it with the image data\n","padded_sequences, label2_one_hot, vocabulary_size = preprocess_text_data(text_data, labels, max_sequence_length)"],"metadata":{"id":"lsnZXdV1wNO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_index = np.max(padded_sequences)\n","vocabulary_size = max_index + 1\n","label2_one_hot.astype"],"metadata":{"id":"tUPY_BHzHqfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","num_image_samples = image_data.shape[0]\n","num_text_samples = padded_sequences.shape[0]\n","\n","\n","X_image_train, X_image_val, X_text_train, X_text_val, y_label1_train, y_label1_val, y_label2_train, y_label2_val = train_test_split(\n","    image_data, padded_sequences, labels_one_hot , label2_one_hot, test_size=0.2, random_state=42\n",")"],"metadata":{"id":"Wn-7qBngs0Gg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Lz_HALKVTES"},"source":["# Combine of model"]},{"cell_type":"code","source":["\n","# CNN model for grayscale images\n","cnn_input_shape = (48,48,3)\n","lstm_input_shape = 31\n","cnn_input = Input(shape=cnn_input_shape)\n","\n","cnn_layer1 = Conv2D(32, (3, 3), activation='relu', padding='same')(cnn_input)\n","cnn_layer2 = MaxPooling2D((2, 2))(cnn_layer1)\n","cnn_layer3 = Conv2D(64, (3, 3), activation='relu', padding='same')(cnn_layer2)\n","cnn_layer4 = MaxPooling2D((2, 2))(cnn_layer3)\n","cnn_layer5 = Conv2D(128, (3, 3), activation='relu', padding='same')(cnn_layer4)\n","cnn_layer6 = MaxPooling2D((2, 2))(cnn_layer5)\n","\n","cnn_gap = GlobalAveragePooling2D()(cnn_layer6)\n","\n","cnn_dense1 = Dense(256, activation='relu')(cnn_gap)\n","cnn_dropout1 = Dropout(0.5)(cnn_dense1)\n","cnn_output = Dense(3, activation='softmax')(cnn_dropout1)\n","\n","# LSTM model\n","lstm_input = Input(shape=lstm_input_shape)\n","embedding = Embedding(input_dim=vocabulary_size, output_dim=100)(lstm_input)\n","lstm_layer1 = LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(embedding)\n","lstm_layer2 = LSTM(32, dropout=0.3, recurrent_dropout=0.3)(lstm_layer1)\n","lstm_batchnorm = BatchNormalization()(lstm_layer2)\n","\n","lstm_output = Dense(3, activation='softmax')(lstm_batchnorm)\n","# Combined architecture\n","model = Model(inputs=[cnn_input, lstm_input], outputs=[cnn_output, lstm_output])\n","model.summary()\n","\n","# Compile and train the model\n","model.compile(optimizer='adam', loss=['categorical_crossentropy', 'categorical_crossentropy'], metrics=['accuracy'])"],"metadata":{"id":"ljPJ4RGPBl7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn_input_shape = (48, 48, 1)\n","lstm_input_shape = 31\n","cnn_input = Input(shape=cnn_input_shape)\n","\n","cnn_layer1 = Conv2D(32, (3, 3), activation='relu', padding='same')(cnn_input)\n","cnn_layer1_bn = BatchNormalization()(cnn_layer1)\n","cnn_layer2 = MaxPooling2D((2, 2))(cnn_layer1_bn)\n","cnn_layer3 = Conv2D(64, (3, 3), activation='relu', padding='same')(cnn_layer2)\n","cnn_layer3_bn = BatchNormalization()(cnn_layer3)\n","cnn_layer4 = MaxPooling2D((2, 2))(cnn_layer3_bn)\n","cnn_layer5 = Conv2D(128, (3, 3), activation='relu', padding='same')(cnn_layer4)\n","cnn_layer5_bn = BatchNormalization()(cnn_layer5)\n","cnn_layer6 = MaxPooling2D((2, 2))(cnn_layer5_bn)\n","\n","cnn_gap = GlobalAveragePooling2D()(cnn_layer6)\n","\n","cnn_dense1 = Dense(256, activation='relu')(cnn_gap)\n","cnn_dropout1 = Dropout(0.5)(cnn_dense1)\n","cnn_output = Dense(3, activation='softmax')(cnn_dropout1)\n","\n","# LSTM model\n","lstm_input = Input(shape=lstm_input_shape)\n","embedding = Embedding(input_dim=vocabulary_size, output_dim=100)(lstm_input)\n","lstm_layer1 = LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(embedding)\n","lstm_layer1_bn = BatchNormalization()(lstm_layer1)\n","lstm_layer2 = LSTM(32, dropout=0.3, recurrent_dropout=0.3)(lstm_layer1_bn)\n","\n","lstm_output = Dense(3, activation='softmax')(lstm_layer2)\n","\n","# Combined architecture\n","model = Model(inputs=[cnn_input, lstm_input], outputs=[cnn_output, lstm_output])\n","model.summary()\n","\n","# Compile and train the model\n","model.compile(optimizer='adam', loss=['categorical_crossentropy', 'categorical_crossentropy'], metrics=['accuracy'])"],"metadata":{"id":"DEp3mvvv0dX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the multi-modal model\n","history = model.fit(\n","    [X_image_train, X_text_train],  # Input data for both branches (images and text)\n","    [y_label1_train, y_label2_train],  # Labels for both output layers (label 1 and label 2)\n","    validation_data=([X_image_val, X_text_val], [y_label1_val, y_label2_val]),  # Validation data\n","    batch_size=15,\n","    epochs=30,  # Set the number of epochs for training\n","    verbose=1  # Set verbosity level (0: silent, 1: progress bar, 2: one line per epoch)\n",")"],"metadata":{"id":"pC1e01HH4fI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/combine_model_30.h5'\n","\n","# Save the model\n","#model.save(model_path)\n","\n","print(\"Model saved successfully.\")"],"metadata":{"id":"yWNvpqaOFlyN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model =load_model(model_path)"],"metadata":{"id":"ABYWFp6Jt70i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation of the model"],"metadata":{"id":"Pv3P8xfa6CZY"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n","import numpy as np"],"metadata":{"id":"t_BUfcZ8BjcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_image_val = X_image_val  # Validation image data\n","X_text_val = X_text_val   # Validation text data\n","\n","# Assuming you have separate true labels for both image and text\n","y_image_val =  y_label1_val  # Validation labels for image category\n","y_text_val =   y_label2_val # Validation labels for text category\n","\n","# #Make predictions using the model\n","y_pred_probabilities = model.predict([X_image_val, X_text_val])\n","\n","# Convert probabilities to binary labels for both image and text\n","y_pred_image = (y_pred_probabilities[0] > 0.5).astype(int)\n","y_pred_text = (y_pred_probabilities[1] > 0.5).astype(int)\n","\n","# Calculate accuracy for both image and text labels\n","accuracy_image = accuracy_score(y_image_val, y_pred_image)\n","accuracy_text = accuracy_score(y_text_val, y_pred_text)\n","\n","print(\"Validation Accuracy for Image Labels:\", int(accuracy_image*100),'%')\n","print(\"Validation Accuracy for Text Labels:\", int(accuracy_text*100),'%')"],"metadata":{"id":"2cv0PTZU6Sfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8gZC6lcyRDUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import hamming_loss, jaccard_score, f1_score\n","\n","# Calculate Hamming loss for image and text labels\n","hamming_loss_image = hamming_loss(y_image_val, y_pred_image)\n","hamming_loss_text = hamming_loss(y_text_val, y_pred_text)\n","\n","# Calculate Jaccard similarity score for image and text labels\n","jaccard_image = jaccard_score(y_image_val, y_pred_image, average='samples')\n","jaccard_text = jaccard_score(y_text_val, y_pred_text, average='samples')\n","\n","# Calculate F1-score for image and text labels\n","f1_score_image = f1_score(y_image_val, y_pred_image, average='samples')\n","f1_score_text = f1_score(y_text_val, y_pred_text, average='samples')\n","\n","print(\"Hamming Loss for Image Labels:\", hamming_loss_image)\n","print(\"Hamming Loss for Text Labels:\", hamming_loss_text)\n","\n","print(\"Jaccard Similarity Score for Image Labels:\", jaccard_image)\n","print(\"Jaccard Similarity Score for Text Labels:\", jaccard_text)\n","\n","print(\"F1-score for Image Labels:\", f1_score_image)\n","print(\"F1-score for Text Labels:\", f1_score_text)\n","\n","# Calculate the number of correct predictions for image labels\n","correct_image_predictions = sum((y_pred_image == y_image_val).all(axis=1))\n","\n","# Calculate the number of correct predictions for text labels\n","correct_text_predictions = sum((y_pred_text == y_text_val).all(axis=1))\n","\n","print(\"Number of Correct Predictions for Image Labels:\", correct_image_predictions)\n","print(\"Number of Correct Predictions for Text Labels:\", correct_text_predictions)\n","\n","# Calculate the number of incorrect predictions for image labels\n","incorrect_image_predictions = len(y_image_val) - correct_image_predictions\n","\n","# Calculate the number of incorrect predictions for text labels\n","incorrect_text_predictions = len(y_text_val) - correct_text_predictions\n","\n","print(\"Number of Incorrect Predictions for Image Labels:\", incorrect_image_predictions)\n","print(\"Number of Incorrect Predictions for Text Labels:\", incorrect_text_predictions)"],"metadata":{"id":"_V5p27Bh-Bdb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting\n","labels = ['Image Labels', 'Text Labels']\n","correct_predictions = [correct_image_predictions, correct_text_predictions]\n","incorrect_predictions = [incorrect_image_predictions, incorrect_text_predictions]\n","\n","x = range(len(labels))\n","\n","plt.bar(x, correct_predictions, label='Correct Predictions', color='green')\n","plt.bar(x, incorrect_predictions, bottom=correct_predictions, label='Incorrect Predictions', color='red')\n","\n","plt.xlabel('Label Type')\n","plt.ylabel('Number of Predictions')\n","plt.title('Number of Correct and Incorrect Predictions')\n","plt.xticks(x, labels)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"NNG8ouDIF0yr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_image_losses = []\n","train_image_accuracies = []\n","train_text_losses = []\n","train_text_accuracies = []\n","\n","# training data\n","image_train_data = X_image_train\n","text_train_data =    X_text_train\n","label_image_train_data = y_label1_train\n","label_text_train_data = y_label2_train\n","\n","\n","# Validation data for both modalities\n","image_val_data = X_image_val\n","text_val_data = X_text_val\n","label_image_val_data = y_label1_val\n","label_text_val_data = y_label2_val\n","\n","# Training loop\n","epochs = 25\n","for epoch in range(epochs):\n","    # Train the multi-modal model\n","    history = model.fit([image_train_data, text_train_data], [label_image_train_data, label_text_train_data],\n","                                    validation_data=([image_val_data, text_val_data],\n","                                                     [label_image_val_data, label_text_val_data]),\n","                                    epochs=1, batch_size=32)\n","\n","    # Append metrics to lists for image modality\n","    train_image_losses.append(history.history['dense_7_loss'][0])\n","    train_image_accuracies.append(history.history['dense_7_accuracy'][0])\n","\n","    # Append metrics to lists for text modality\n","    train_text_losses.append(history.history['dense_8_loss'][0])\n","    train_text_accuracies.append(history.history['dense_8_accuracy'][0])\n","\n","    # Plot performance curves using scatter plots\n","    plt.figure(figsize=(10, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.scatter(range(epoch + 1), train_image_losses, label='Train Image Loss', marker='o')\n","    plt.scatter(range(epoch + 1), train_text_losses, label='Train Text Loss', marker='o')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(range(epoch + 1), train_image_accuracies, label='Train Image Accuracy', marker='o')\n","    plt.scatter(range(epoch + 1), train_text_accuracies, label='Train Text Accuracy', marker='o')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(f'performance_plots_epoch_{epoch + 1}.png')  # Save plot as an image\n","    plt.close()\n","\n","# Save the performance data to a CSV file\n","data = list(zip(range(epochs), train_image_losses, train_image_accuracies, train_text_losses, train_text_accuracies))\n","csv_file = 'performance_data.csv'\n","with open(csv_file, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Epoch', 'Train Image Loss', 'Train Image Accuracy', 'Train Text Loss', 'Train Text Accuracy'])\n","    writer.writerows(data)\n","\n","print(f'Data saved to {csv_file}')"],"metadata":{"id":"t904i_AirZix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image = precision_score(y_image_val, y_pred_image, average = 'micro')\n","Text = precision_score(y_text_val, y_pred_text, average = 'micro')\n","\n","# Calculate accuracy\n","#accuracy = accuracy_score(true_labels, predicted_classes)\n","\n","# Calculate precision\n","#precision = , predicted_classes, average='weighted')\n","\n","# Calculate F1 score\n","#f1 = f1_score(true_labels, predicted_classes, average='weighted')\n","\n","#print(\"Confusion Matrix:\")\n","print(\"image_precision_score\",image)\n","print(\"text_precision_score\",Text)\n","#print(\"Accuracy:\", accuracy)\n","#print(\"Precision:\", precision)\n","#print(\"F1 Score:\", f1)"],"metadata":{"id":"d16-9-mi_q4V"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ysMHTx19pEV7yssB3QX97t2cBI8c2r0E","timestamp":1693499407850},{"file_id":"1WZRCiNWu03KKu_T_hGz-p-A5B66i5-aF","timestamp":1689092298203}],"gpuType":"T4","private_outputs":true,"collapsed_sections":["jsYsmBD0hsbG","vtVCij9tibtc"],"mount_file_id":"1OJ7Jp5o309gK5MXV_MFEzOXpfYDrkiF4","authorship_tag":"ABX9TyNJ7yXtFxE8WKh4KJcYUe9E"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}